{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratory Notebook"
   ]
  },
  {
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [24/03/2021 Preliminary research](#preliminary)\n",
    "2. [28/03/2021 Writing the abstract](#abstract)\n",
    "3. [29/03/2021 Full reading of the DOI Handbook](#handbook)\n",
    "4. [03/04/2021 Data Management Plan Version 1.0](#dmp1)\n",
    "5. [09/04/2021 Literature review](#literature)\n",
    "6. [11/04/2021 Computational workflow Version 1.0](#workflow1)\n",
    "7. [17/04/2021 Open peer review](#peerReview)\n",
    "8. [17/04/2021 First version of the regular expression to clean DOI's wrong prefixes](#regex1)\n",
    "9. [20/04/2021 Reflections on Open Source Licenses](#license)\n",
    "10. [20/04/2021 Code to handle HTTP requests](#requests)\n",
    "11. [22/04/2021 Improvements to the regex to clean up prefix errors and code modularization](#modularization)\n",
    "12. [23/04/2021 Final algorithm design](#final_algorithm)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24/03/2021 Preliminary research <a name=\"preliminary\"></a>\n",
    "\n",
    "<p>Today I started working on the problem of wrong DOI names reported by Crossref. In particular, the research question on which the entire work focuses is which are the classes of errors that characterize the invalid DOI names and which classes can be addressed through automatic processes in order to obtain the correct DOI names.<p/>\n",
    "<p>Having never used the DOI API, I explored the subject by reading its documentation<sup><a href=\"#preliminary_ref_01\">[1]</a></sup>. In particular, in chapter <em>3.8.3 Proxy Server REST API</em> I discovered that by performing a GET to https://doi.org/api/handles/&lt;doi&gt; it is possible to obtain useful information in response. Among these, the status code is particularly interesting, which can take on four values:\n",
    "<ul>\n",
    "    <li>1: Success. (HTTP 200 OK)</li>\n",
    "    <li>2: Error. Something unexpected went wrong during handle resolution. (HTTP 500 Internal Server Error)</li>\n",
    "    <li>100: Handle Not Found. (HTTP 404 Not Found)</li>\n",
    "    <li>200: Values Not Found. The handle exists but has no values (or no values according to the types and indices specified). (HTTP 200 OK)</li>\n",
    "</ul>\n",
    "This is extremely useful for identifying those initially invalid DOI names that have become valid in the meantime.</p>\n",
    "   \n",
    "### References\n",
    "<ol>\n",
    "    <li id=\"preliminary_ref_01\">International DOI Foundation. (2019). DOI® Handbook. <a href=\"https://doi.org/10.1000/182\" target=\"_blank\">https://doi.org/10.1000/182</a>.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28/03/2021 Writing the abstract <a name=\"abstract\"></a>\n",
    "\n",
    "\n",
    "<p>Together with my colleagues Cristian Santini (orcid: 0000-0001-7363-6737), Ricarda Boente (orcid: 0000-0002-2133-8735) and Deniz Tural (orcid: 0000-0002-6391-4198) I wrote the first version of the abstract.</p>\n",
    "<p>Starting from the initial hypothesis that there are two classes of errors, namely factual errors and DOI names that are not yet valid at the time of processing, we tried to further break down the first class into various subclasses: exploring the input dataset, we hypothesized that a DOI name may be factually wrong because it contains forbidden characters, because it contains excess strings at the beginning or at the end or due to a human error in the transcription.</p>\n",
    "<p>Moreover, I have elaborated a first hypotheses on how to deal with both errors' classes. As for the first class, I speculated that it would be possible to obtain the correct cited DOI names starting from the valid citing DOI names, using the REST API for COCI: to obtain through the COCI references operation all the cited articles and, using word embeddings algorithm, to identify the most similar DOI name to the wrong one. However, my colleagues have rightly pointed out that wrong DOI names are not accepted by COCI and that COCI is built from Crossref, so it would not be possible to get the correct DOI names from COCI. Another hypothesis proposed by Cristian was to exploit other metadata provided by Crossref relating to that particular wrong DOI name to check if there are other DOI names connected to it, which perhaps refer to the correct one. Finally, it was decided to remain as vague as possible in the abstract and to summarize the various hypotheses formulated in the expression \"rule-based methods\" with the idea of defining this point after further research.<p/>\n",
    "<p>Instead, there has been much more consensus on the strategy for dealing with the second class of errors, namely those due to DOI names that are not yet valid, that is, use the DOI API by interpreting the response status code.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 29/03/2021 Full reading of the DOI Handbook <a name=\"handbook\"></a>\n",
    "\n",
    "<p>Writing the abstract raised some questions about the specifics of a DOI name. Therefore, I decided to read the entire documentation with the aim of answering three questions:\n",
    "<ol>\n",
    "    <li>Do DOI names use a limited character set?</li>\n",
    "    <li>Do DOI names have semantics?</li>\n",
    "    <li>In which ways can I query the DOI System Proxy Server?</li>\n",
    "</ol>\n",
    "<p>Chapter <em>2.5.1 Encoding principle</em> answers the first question:</p>\n",
    "<blockquote>\"DOI names may incorporate any printable characters from the Universal Character Set (UCS-2), of ISO / IEC 10646, which is the character set defined by Unicode. The character set encompasses most characters used in every major language written today. However, because of specific uses made of certain characters by some Internet technologies (the use of pointed brackets in xml for example), there may some effective restrictions in day-to-day use.\"</blockquote>\n",
    "<p>Chapter <em>2.2.1 General characteristics</em> answers the second question:</p>\n",
    "<blockquote>The DOI name is an opaque string for the purposes of the DOI system. No definitive information may be inferred from the specific character string of a DOI name. In particular, the inclusion in a DOI name of any registrant code allocated to a specific registrant does not provide evidence of the ownership of rights or current management responsibility of any intellectual property in the referent. Such information may be asserted in the associated metadata.</blockquote>\n",
    "<p>As for the third question, chapter <em>3.8.4.2 Parameter Passing</em> explores a series of queries that can be performed to specify the desired output. Therefore, I hypothesized that among the wrong DOI names there could be some that, during the extraction phase, had not been deprived of the query portion. The hypothesis was confirmed. The code used to perform this check is shown below.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The wrong DOI names that contain queries are 228 out of 1223297\nThe number of queries found is equal to 39. They are:\n{'?refreqid=', '?genre=', '?abstract_id=', '?accountid=', '?scroll=', '?error=', '?prd=', '?nosfx=', '?arnumber=', '?src=', '?articleid=', '?locale=', '?seq=', '?ver=', '?id=', '?doid=', '?rss=', '?term=', '?page=', '?uid=', '?slug=', '?report=', '?ref=', '?code=', '?download=', '?goto=', '?sequence=', '?select-row=', '?doi=', '?rskey=', '?no-access=', '?artid=', '?origin=', '?v=', '?site=', '?print=', '?crawler=', '?title=', '?sid='}\n"
     ]
    }
   ],
   "source": [
    "import csv, re, urllib.request\n",
    "\n",
    "url = 'https://zenodo.org/record/4625300/files/invalid_dois.csv'\n",
    "response = urllib.request.urlopen(url)\n",
    "lines = [l.decode('utf-8') for l in response.readlines()]\n",
    "reader = csv.reader(lines)\n",
    "rows_number = 0\n",
    "proxy_server_occurrences = list()\n",
    "proxy_server_queries = set()\n",
    "for row in reader:\n",
    "    rows_number += 1\n",
    "    query = re.search(\"\\?.+?=\", row[1])\n",
    "    if query is not None:\n",
    "        query = query.group(0)\n",
    "        proxy_server_occurrences.append(query)\n",
    "        proxy_server_queries.add(query)\n",
    "print(f\"The wrong DOI names that contain queries are {len(proxy_server_occurrences)} out of {rows_number}\")\n",
    "print(f\"The number of queries found is equal to {len(proxy_server_queries)}. They are:\")\n",
    "print(proxy_server_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03/04/2021 Data Management Plan Version 1.0 <a name=\"dmp1\"></a>\n",
    "\n",
    "<p>Together with my colleagues I compiled version 1.0 of the Data Management Plan, using the ARGOS platform by OpenAIRE and EUDAT. Two datasets were considered: one for the output and the other for the code. Particular care was taken in filling in as many fields as possible, but some doubts arose about which metadata to use in order to comply with the FAIR principles<sup><a href=\"#dmp1_ref_01\">[1]</a><a href=\"#dmp1_ref_02\">[2]</a><a href=\"#dmp1_ref_03\">[3]</a></sup>, an aspect that will be further explored later.</p>\n",
    "\n",
    "### References\n",
    "<ol>\n",
    "    <li id=\"dmp1_ref_01\">Wilkinson, M. D., Dumontier, M., Aalbersberg, I. J., Appleton, G., Axton, M., Baak, A., Blomberg, N., Boiten, J.-W., da Silva Santos, L. B., Bourne, P. E., Bouwman, J., Brookes, A. J., Clark, T., Crosas, M., Dillo, I., Dumon, O., Edmunds, S., Evelo, C. T., Finkers, R., … Mons, B. (2016). The FAIR Guiding Principles for scientific data management and stewardship. Scientific Data, 3, 160018. https://doi.org/10.1038/sdata.2016.18.</li>\n",
    "    <li id=\"dmp1_ref_02\">GO FAIR. (2018). FAIR Principles. https://www.go-fair.org/fair-principles/.</li>\n",
    "    <li id=\"dmp1_ref_03\">Michener, W. K. (2015). Ten Simple Rules for Creating a Good Data Management Plan. PLOS Computational Biology, 11(10), e1004525. https://doi.org/10.1371/journal.pcbi.1004525.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 09/04/2021 Literature review <a name=\"literature\"></a>\n",
    "<p>Taking a cue from Cristian's laboratory notebook, I read the articles he reported on 09/04/2021 about past studies on our research topic. I found several interesting ideas in <em>Types of DOI errors of cited references in Web of Science with a cleaning method</em><sup><a href=\"#literature_ref_01\">[1]</a></sup> by Shuo Xu et al. :</p>\n",
    "<ul>\n",
    "    <li>The study suggests that there are three types of DOI errors, namely prefix-, suffix- and other-type errors. The other-type errors are further divided into three subgroups: (a) those containing special characters (such as 10.1034/j.1600-0404.2000.101004262x./), (b) incoherently described DOIs (such as 10.1038/), and (c) those with incomplete suffix but with correct DOI prefix (such as 10.1007/3-540-48194-X_).</li>\n",
    "    <li>Three regular expressions are proposed to clean up the respective types of errors.\n",
    "        <img src=\"https://media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs11192-019-03162-4/MediaObjects/11192_2019_3162_Fig2_HTML.png?as=webp\"/>\n",
    "It is worth mentioning that the aforementioned regular expressions are applied to strings already cleaned of double underscores, double periods, XML tags, spaces and forward slashes.\n",
    "    </li>\n",
    "    <li>Finally, the article mentions some problems that cannot be solved by this approach: (a) similar characters confused with each other, such as “O” versus “0”, “b” versus “6” and “O” versus “Q”; (b) to distinguish the correct DOI name from multiple DOI names assigned to the same cited reference; (c) a DOI name assigned to some cited reference that cannot be resolved by the DOI system; (d) a DOI name that is resolvable, but points to some knowledge unit within the interested cited reference.</li>\n",
    "</ul>\n",
    "<p>Also, the article <em>DOI errors and possible solutions for Web of Science</em> by Juenwen Zhu et al.<sup><a href=\"#literature_ref_02\">[2]</a></sup> suggests that DOI names often contain the wrong characters for similarity to the right ones, such as \"O\" instead of \"0\", \"b\" instead of \"6\", \"O\" instead of \"Q\". A possible solution could then be to apply these replacements and verify the new DOI is resolved by the DOI System Proxy.</p>\n",
    "<p>Finally, the paper <em>Errors in DOI indexing by bibliometrics databases</em> by Fiorenzo Franceschini et al.<sup><a href=\"#literature_ref_03\">[3]</a></sup> reports a more generic classification of bibliographic database errors, i.e. it distinguishes between authors' errors in creating the list of cited resources, and database mapping errors, such as transcription errors. It is clear that our work focuses only on the second category. The article then proceeds with analyzing a further category of error, i.e. single DOI names associated with different papers. However, no solution is proposed to the problem, which is only highlighted.</p>\n",
    "\n",
    "## References\n",
    "<ol>\n",
    "    <li id=\"literature_ref_01\">Xu, S., Hao, L., An, X. et al. Types of DOI errors of cited references in Web of Science with a cleaning method. Scientometrics 120, 1427–1437 (2019). https://doi.org/10.1007/s11192-019-03162-4.</li>\n",
    "    <li id=\"literature_ref_02\">Zhu, J., Hu, G. &amp; Liu, W. DOI errors and possible solutions for Web of Science. Scientometrics 118, 709–718 (2019). https://doi.org/10.1007/s11192-018-2980-7.</li>\n",
    "    <li id=\"literature_ref_03\">Franceschini, F., Maisano, D., &amp; Mastrogiacomo, L. (2015). Errors in DOI indexing by bibliometric databases. Scientometrics, 102(3), 2181–2186. https://doi.org/10.1007/s11192-014-1503-4.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11/04/2021 Computational workflow Version 1.0 <a name=\"workflow1\"></a>\n",
    "\n",
    "<p>The first version of the computational workflow, that is the protocol, was carried out collectively by all members of the group from the beginning to the end.</p>\n",
    "<p>The platform chosen for creating, editing and publishing the protocol was Protocols.io (<a href=\"https://www.protocols.io/welcome\" alt=\"Protocols.io home page\" target=\"_blank\">https://www.protocols.io/welcome</a>).</p>\n",
    "<p>Having found valid ideas from the study of the existing literature, for the first version we decided to structure the protocol by reusing already existing methods, in particular as regards the classification of possible DOI errors (Buchanan, 2006) and as regards the cleaning of strings using regular expressions (Xu et al., 2019).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17/04/2021 Open peer review <a name=\"peerReview\"></a>\n",
    "<p>In order to better understand how to carry out and structure a peer review, I read the article <em>How to write a thorough peer review</em> (Column, 2018), which inspired me about the <em>modus operandi</em> as well as clarifying the spirit in which a review should be drafted. Specifically, having to review a computational workflow, I also deepened the <em>Guidelines for Reviewers</em> provided by PLOS ONE, which concern not only the protocols but in general the dynamics related to the review process.</p>\n",
    "<p>Having those premises, I tried to stick to both guidelines, taking three readings of the protocol, taking notes away and focusing on different aspects each time. I also tried to answer four questions:</p>\n",
    "<ul>\n",
    "    <li>Does the manuscript provide valid rationale for the planned or ongoing study, with clearly identified and justified research questions?</li>\n",
    "    <li>Is the protocol technically sound and planned in a manner that will lead to a meaningful outcome and allow testing of the stated hypotheses?</li>\n",
    "    <li>Have the authors described where all data underlying the findings will be made available when the study is complete?</li>\n",
    "    <li>Is the methodology feasible and does the description provide sufficient methodological detail for the protocol to be reproduced and replicated?</li>\n",
    "</ul>\n",
    "<p>Therefore, I organized the review in 4 chapters:</p>\n",
    "<ul>\n",
    "    <li><strong>The premises. About the study's rationale and impact</strong></li>\n",
    "    <li><strong>The methodology. About the protocol's technical soundness</strong></li>\n",
    "    <li><strong>The reproducibility. About the the input and output</strong></li>\n",
    "    <li><strong>Conclusions</strong></li>\n",
    "</ul>\n",
    "\n",
    "## References\n",
    "<ol>\n",
    "    <li>Stiller-Reeve, M. (2018). How to write a thorough peer review. Nature. https://doi.org/10.1038/d41586-018-06991-0.</li>\n",
    "    <li>Guidelines for Reviewers. PLOS ONE. https://journals.plos.org/plosone/s/reviewer-guidelines.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17/04/2021 First version of the regular expression to clean DOI's wrong prefixes <a name=\"regex1\"></a>\n",
    "<p>Trying to use the regular expression to clean up wrong DOI prefixes proposed in <em>Types of DOI errors of cited references in Web of Science with a cleaning method</em> (Zu, Shuo et al., 2019), I quickly realized that it would never could have matched. The regular expression is the following:</p>\n",
    "<p><code>^(?:D[0|O]I\\/?HTTP:\\/\\/DX.D[0|O]I.[0|O]RG\\/[0|O]RG\\/[:|\\/]\\\\\\\\d+\\\\\\\\.HTTP:\\/\\/DX.D[0|O]I.[0|O]RG\\/?)+(.*)</code></p>\n",
    "<p>The reasons why it cannot match are the following:</p>\n",
    "<ol>\n",
    "    <li>The various protocols and domain names are all mandatory, none optional, while it is more plausible that a match will occur if they are reported as optional with the exclamation mark.</li>\n",
    "    <li>The match does not necessarily occur at the beginning of the string. For example, in the following incorrect DOI taken from the input dataset, the protocol is in the middle of the string and not at the beginning: <code>10.2478/s11696-009-0027-5,10.1016/j.aca.2006.07.086.http://dx.doi.org/10.1016/j.aca.2006.07.086\"</code>.</li>\n",
    "</ol>\n",
    "<p>Therefore, I have written a new version of the regular expression, which takes these two aspects into account, and it is the following:</p>\n",
    "<p><code>(?:http:\\/\\/dx.d[0|o]i.[0|o]rg\\/)+(.*)</code></p>\n",
    "<p>It is expected to further refine it shortly.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20/04/2021 Reflections on Open Source Licenses <a name=\"license\"></a>\n",
    "\n",
    "<p>I have read several Open Source Licenses and, from what I understand, there are two types: the permissive ones (Apache License 2.0, BSD, MIT license, ISC) and the ones with the copyleft (GPL). Assuming that I had a hard time finding substantial differences between the various permissive licenses, I like the Apache License 2.0 more than the others, because the it is more explicit about what it means by every single term used, such as \"License\", \"Licensor\", \"Legal Entity\" and \" You \".</p>\n",
    "<p>However, in general, the GPL license seems to be the one that takes the concept of open-source more seriously, ensuring that every single redistribution of the source remains open-source. That way, even if a derivative work is distributed for a fee, by the time someone legally comes into possession of that product, the code would have to be released under the GPL. For this reason, I imagine the GPL is less popular than the other licenses, as it restricts the potential users of the software. However, this kind of limitation seems ethical to me, and since Science and Openness are the focus for this course and project, it seems that GPL would be a more relevant choice as a license.</p>\n",
    "\n",
    "## References\n",
    "\n",
    "1. Licenses & Standards. Open Source Iniziative. https://opensource.org/licenses.\n",
    "2. Open Source Initiative. (2007). The Open Source Definition. https://opensource.org/osd. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20/04/2021 Code to handle HTTP requests <a name=\"requests\"></a>\n",
    "\n",
    "Trying to check the validity of the over million DOIs present in the input dataset, my colleague Ricarda Boente realized that the execution times were extremely long, measurable in days, and that the execution often crashed before reaching the end.\n",
    "\n",
    "In order to avoid future crashes, I have implemented a helper function, handle_request, which takes care of handling all exceptions that might occur during an http request, such as server-side (500, 502, 504, 520, 521) and client-side errors (404). In case of server-side errors, the function waits for the response for 60 seconds, then makes a second attempt. If the second fails, cancel the request and save the error in a dictionary. This way, you can export a log file at the end of the run in case there were any errors. Moreover, all requests are saved in a cache file, so they never have to be made again in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\users\\arcan\\appdata\\roaming\\python\\python39\\site-packages (2.25.1)\n",
      "Requirement already satisfied: requests_cache in c:\\users\\arcan\\appdata\\roaming\\python\\python39\\site-packages (0.5.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\arcan\\appdata\\roaming\\python\\python39\\site-packages (from requests) (1.26.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\arcan\\appdata\\roaming\\python\\python39\\site-packages (from requests) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arcan\\appdata\\roaming\\python\\python39\\site-packages (from requests) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\arcan\\appdata\\roaming\\python\\python39\\site-packages (from requests) (2.10)\n",
      "WARNING: You are using pip version 20.2.3; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the 'c:\\program files\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install requests requests_cache\n",
    "\n",
    "import requests, requests_cache, json\n",
    "from requests import Session\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "class Support(object):\n",
    "    def _requests_retry_session(\n",
    "        tries=1,\n",
    "        status_forcelist=(500, 502, 504, 520, 521),\n",
    "        session=None\n",
    "    ) -> Session:\n",
    "        session = session or requests.Session()\n",
    "        retry = Retry(\n",
    "            total=tries,\n",
    "            read=tries,\n",
    "            connect=tries,\n",
    "            status_forcelist=status_forcelist,\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry)\n",
    "        session.mount('http://', adapter)\n",
    "        session.mount('https://', adapter)\n",
    "        return session\n",
    "\n",
    "    def handle_request(self, url:str, cache_path:str=\"\", error_log_dict:dict=dict()):\n",
    "        if cache_path != \"\":\n",
    "            requests_cache.install_cache(cache_path)\n",
    "        try:\n",
    "            data = self._requests_retry_session().get(url, timeout=60)\n",
    "            if data.status_code == 200:\n",
    "                return data.json()\n",
    "            else:\n",
    "                error_log_dict[url] = data.status_code\n",
    "        except Exception as e:\n",
    "            error_log_dict[url] = str(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22/04/2021 Improvements to the regex to clean up prefix errors and code modularization<a name=\"modularization\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I further refined the regular expression to capture prefix errors, in order to also capture the prefixes related to the new Proxy Server address, i.e. https://doi.org/, which was not considered by the regex taken from the aforementioned paper (Zu, Shuo et al., 2019). The refined regular expression is therefore:\n",
    "\n",
    "<p><code>\"(.*?)(?:\\.)?(?:http:\\/\\/dx\\.d[0|o]i\\.[0|o]rg\\/|https:\\/\\/doi\\.org\\/)(.*)\"</code></p>\n",
    "\n",
    "The new regex was able to catch over 22,000 errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The regular expression matched 22077 DOIs, for example:\n[   {   'Cleaned_prefix_DOI': '10.1016/j.aca.2006.07.086',\n        'Invalid_DOI': '10.1016/j.aca.2006.07.086.http://dx.doi.org/10.1016/j.aca.2006.07.086'},\n    {   'Cleaned_prefix_DOI': '10.1016/j.jlumin.2004.10.018',\n        'Invalid_DOI': '10.1016/j.jlumin.2004.10.018.http://dx.doi.org/10.1016/j.jlumin.2004.10.018'},\n    {   'Cleaned_prefix_DOI': '10.1016/0014-4894(72)90103-8',\n        'Invalid_DOI': '10.1016/0014-4894(72)90103-8.http://dx.doi.org/10.1016/0014-4894(72)90103-8'},\n    {   'Cleaned_prefix_DOI': '10.1002/app.23704',\n        'Invalid_DOI': '10.1002/app.23704.http://dx.doi.org/10.1002/app.23704'},\n    {   'Cleaned_prefix_DOI': '10.1006/jcat.2001.3474',\n        'Invalid_DOI': '10.1006/jcat.2001.3474.http://dx.doi.org/10.1006/jcat.2001.3474'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pprint\n",
    "\n",
    "def clean_prefixes(data:list, field:str) -> list:\n",
    "    cleaned_prefixes_dois = list()\n",
    "    prefixes_regex = \"(.*?)(?:\\.)?(?:http:\\/\\/dx\\.d[0|o]i\\.[0|o]rg\\/|https:\\/\\/doi\\.org\\/)(.*)\"\n",
    "    for row in data:\n",
    "        invalid_doi = row[field]\n",
    "        match = re.search(prefixes_regex, invalid_doi, re.IGNORECASE)\n",
    "        if match:\n",
    "            new_doi = match.group(1) if match.group(1) > match.group(2) else match.group(2)\n",
    "            cleaned_prefix_doi = {\"Invalid_DOI\": row[\"Invalid_cited_DOI\"], \"Cleaned_prefix_DOI\": new_doi}\n",
    "            cleaned_prefixes_dois.append(cleaned_prefix_doi)\n",
    "    return cleaned_prefixes_dois\n",
    "\n",
    "import csv, re, urllib.request\n",
    "\n",
    "reader = csv.DictReader(lines)\n",
    "cleaned_prefixes_dois = clean_prefixes(data=reader, field=\"Invalid_cited_DOI\")\n",
    "print(f\"The regular expression matched {len(cleaned_prefixes_dois)} DOIs, for example:\")\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(cleaned_prefixes_dois[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also reorganized the code, in order to make it more modular, reusable and maintainable. Therefore, I defined a new Clean_DOIs class, which contains all the methods to validate and clean DOIs, and a second Support class, which takes care of handling inputs, outputs and requests."
   ]
  },
  {
   "source": [
    "## 23/04/2021 Final algorithm design<a name=\"final_algorithm\"></a>\n",
    "\n",
    "<p>I reorganized all the code written by my colleagues Cristian, Ricarda and Deniz into a single final algorithm, divided into three methods of the Clean_DOIs class, namely <strong>check_dois_validity</strong>, <strong>procedure</strong> and <strong>clean_doi</strong>.</p> \n",
    "<ul>\n",
    "    <li>The check_dois_validity method verifies how many DOIs of the input dataset belong   to the class of previously invalid DOIs that have become valid, through requests to the DOI Proxy Server, and outputs a list of dictionaries, each consisting of 4 keys: \"Valid_citing_DOI\", which contains the valid citing DOI in input, \"Invalid_cited_DOI\", which contains the invalid cited DOI in input, \"Valid_DOI\", which contains the valid DOI if the invalid DOI is actually valid or an empty string otherwise, and \"Already_valid\", which is equal to one if the previous case is true, 0 if it is false.</li>\n",
    "    <li>The clean_doi method applies the regular expressions to clean prefixes and suffixes and corrects the other-type errors sequentially, saving which class of error was found in a dictionary. This dictionary is eventually returned along with the correct DOI.</li>\n",
    "    <li>Finally, the procedure method takes care of executing the clean_doi method on each row of the input dictionaries list and verifying if the new DOI is valid through the DOI Proxy Server, finally returning the actual output, consisting of a list of 7-key dictionaries: \"Valid_citing_DOI\", \"Invalid_cited_DOI\", \"Valid_DOI\" and \"Already_valid\", taken from the output of the check_dois_validity method, and \"Prefix_error\", \"Suffix_error\" and \"Other-type_error\", which correspond to 1 if the found error belongs to that class, 0 otherwise.</li>\n",
    "</ul>\n",
    "<p>It is worth noting that the check_dois_validity and procedure methods both cycle over the input dataset in an apparently inefficient way because the entire workflow could be completed in one cycle. However, the disadvantages of carrying out the entire process in one cycle are greater than the advantages, for two reasons:</p>\n",
    "<ul>\n",
    "    <li>The procedure, being applied to more than one million DOIs, takes a long time. Dividing it into two steps allows to save the results of the first phase and start from the second without having to repeat everything from the beginning.</li>\n",
    "    <li>The second phase does not repeat the requests to the DOI Proxy Server made by the first, but makes new ones only if the first phase has returned 0 to the \"Already_valid\" field and if the clean DOI is different from the invalid one. Therefore, the advantages of dividing the process are greater than the disadvantages.</li>\n",
    "</ul>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class Clean_DOIs(object):\n",
    "    def __init__(self, cache_path:str=\"\", logs:dict={}):\n",
    "        self.cache_path = cache_path\n",
    "        self.logs = logs\n",
    "        self.prefix_regex = \"(.*?)(?:\\.)?(?:HTTP:\\/\\/DX\\.D[0|O]I\\.[0|O]RG\\/|HTTPS:\\/\\/D[0|O]I\\.[0|O]RG\\/)(.*)\"\n",
    "        suffix_dcsupplemental = \"\\/-\\/DCSUPPLEMENTAL\"\n",
    "        suffix_suppinfo = \"SUPPINF[0|O](\\.)?\"\n",
    "        suffix_pmid1 = \"[\\.|\\(|,|;]?PMID:\\d+.*?\"\n",
    "        suffix_pmid2 = \"[\\.|\\(|,|;]?PMCID:PMC\\d+.*?\"\n",
    "        suffix_epub = \"[\\(|\\[]EPUBAHEADOFPRINT[\\)\\]]\"\n",
    "        suffix_published_online = \"[\\.|\\(|,|;]?ARTICLEPUBLISHEDONLINE.*?\\d{4}\"\n",
    "        suffix_http = \"[\\.|\\(|,|;]*HTTP:\\/\\/.*?\"\n",
    "        suffix_subcontent = \"\\/(META|ABSTRACT|FULL|EPDF|PDF|SUMMARY)([>|\\)](LAST)?ACCESSED\\d+)?\"\n",
    "        suffix_accessed = \"[>|\\)](LAST)?ACCESSED\\d+\"\n",
    "        suffix_sagepub = \"[\\.|\\(|,|;]?[A-Z]*\\.?SAGEPUB.*?\"\n",
    "        suffix_dotted_line = \"\\.{5}.*?\"\n",
    "        suffix_delimiters = \"[\\.|,|<|&|\\(|;]+\"\n",
    "        suffix_double_doi = \"[\\.|\\(|,|;]10.\\d{4}\\/.*?\"\n",
    "        suffix_doi_mark = \"\\[DOI\\].*?\"\n",
    "        suffix_year = \"\\(\\d{4}\\)?\"\n",
    "        suffix_query = \"\\?.*?=.*?\"\n",
    "        suffix_hash = \"#.*?\"\n",
    "        suffix_regex_lst = [suffix_dcsupplemental, suffix_suppinfo, suffix_pmid1, suffix_pmid2, suffix_epub,\n",
    "                            suffix_published_online, suffix_http, suffix_subcontent, suffix_accessed, suffix_sagepub,\n",
    "                            suffix_dotted_line, suffix_delimiters, suffix_double_doi, suffix_doi_mark, suffix_year,\n",
    "                            suffix_query, suffix_hash]\n",
    "        self.suffix_regex = \"(.*?)(?:\" + \"|\".join(suffix_regex_lst) + \")$\"\n",
    "\n",
    "    def check_dois_validity(self, data:list) -> list:\n",
    "        checked_dois = list()\n",
    "        for row in data:\n",
    "            invalid_dictionary = {\n",
    "                \"Valid_citing_DOI\": row[\"Valid_citing_DOI\"],\n",
    "                \"Invalid_cited_DOI\": row[\"Invalid_cited_DOI\"], \n",
    "                \"Valid_DOI\": \"\",\n",
    "                \"Already_valid\": 0\n",
    "            }\n",
    "            handle = Support().handle_request(url=f\"https://doi.org/api/handles/{row['Invalid_cited_DOI']}\", cache_path=self.cache_path, error_log_dict=self.logs)\n",
    "            if handle is not None:\n",
    "                if handle[\"responseCode\"] == 1:\n",
    "                    checked_dois.append(\n",
    "                        {\"Valid_citing_DOI\": row[\"Valid_citing_DOI\"],\n",
    "                        \"Invalid_cited_DOI\": row[\"Invalid_cited_DOI\"], \n",
    "                        \"Valid_DOI\": row['Invalid_cited_DOI'],\n",
    "                        \"Already_valid\": 1\n",
    "                        })\n",
    "                else:\n",
    "                    checked_dois.append(invalid_dictionary)       \n",
    "            else:\n",
    "                checked_dois.append(invalid_dictionary)             \n",
    "        return checked_dois\n",
    "    \n",
    "    def procedure(self, data:list) -> list:\n",
    "        output = list()\n",
    "        for row in data:\n",
    "            invalid_doi = row[\"Invalid_cited_DOI\"]\n",
    "            invalid_dictionary = {\n",
    "                \"Valid_citing_DOI\": row[\"Valid_citing_DOI\"],\n",
    "                \"Invalid_cited_DOI\": row[\"Invalid_cited_DOI\"],\n",
    "                \"Valid_DOI\": row[\"Valid_DOI\"],\n",
    "                \"Already_valid\": row[\"Already_valid\"],\n",
    "                \"Prefix_error\": 0,\n",
    "                \"Suffix_error\": 0,\n",
    "                \"Other-type_error\": 0\n",
    "            }\n",
    "            if row[\"Already_valid\"] != 1:\n",
    "                new_doi, classes_of_errors = self.clean_doi(row[\"Invalid_cited_DOI\"])\n",
    "                valid_dictionary = {\n",
    "                    \"Valid_citing_DOI\": row[\"Valid_citing_DOI\"],\n",
    "                    \"Invalid_cited_DOI\": row[\"Invalid_cited_DOI\"],\n",
    "                    \"Valid_DOI\": new_doi,\n",
    "                    \"Already_valid\": row[\"Already_valid\"],\n",
    "                    \"Prefix_error\": classes_of_errors[\"prefix\"],\n",
    "                    \"Suffix_error\": classes_of_errors[\"suffix\"],\n",
    "                    \"Other-type_error\": classes_of_errors[\"other-type\"]\n",
    "                }\n",
    "                if new_doi != row[\"Invalid_cited_DOI\"]:\n",
    "                    handle = Support().handle_request(url=f\"https://doi.org/api/handles/{new_doi}\", cache_path=self.cache_path, error_log_dict=self.logs)\n",
    "                    if handle is not None:\n",
    "                        output.append(valid_dictionary)\n",
    "                    else:\n",
    "                        output.append(invalid_dictionary)\n",
    "                else:\n",
    "                    output.append(invalid_dictionary)\n",
    "            else:\n",
    "                output.append(invalid_dictionary)\n",
    "        return output\n",
    "    \n",
    "    def clean_doi(self, doi:str) -> str:\n",
    "        tmp_doi = doi.upper().replace(\" \", \"\")\n",
    "        prefix_match = re.search(self.prefix_regex, tmp_doi)\n",
    "        classes_of_errors = {\n",
    "            \"prefix\": 0,\n",
    "            \"suffix\": 0,\n",
    "            \"other-type\": 0\n",
    "        }\n",
    "        if prefix_match:\n",
    "            tmp_doi = prefix_match.group(1)\n",
    "            classes_of_errors[\"prefix\"] += 1\n",
    "        suffix_match = re.search(self.suffix_regex, tmp_doi)\n",
    "        if suffix_match:\n",
    "            tmp_doi = suffix_match.group(1)\n",
    "            classes_of_errors[\"suffix\"] += 1\n",
    "        new_doi = re.sub(\"\\\\\\\\\", \"\", tmp_doi)\n",
    "        new_doi = re.sub(\"__\", \"_\", tmp_doi)\n",
    "        new_doi = re.sub(\"\\\\.\\\\.\", \".\", tmp_doi)\n",
    "        new_doi = re.sub(\"<.*?>.*?</.*?>\", \"\", tmp_doi)\n",
    "        new_doi = re.sub(\"<.*?/>\", \"\", tmp_doi)\n",
    "        if new_doi != tmp_doi:\n",
    "            classes_of_errors[\"other-type\"] += 1\n",
    "        return new_doi, classes_of_errors\n"
   ]
  },
  {
   "source": [
    "Finally, I created a tutorial.py program, which illustrates through comments how to perform the entire procedure from the beginning to the end and executes it, for the benefit of reproducibility. The current version can be found at the following address: https://github.com/open-sci/2020-2021-grasshoppers-code/blob/e50e52f728f442cea35bdf72641e644f02d88fea/tutorial.py. It is not possible to run it similarly on Jupyter Notebook, but I still want to show a sample of the final output."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import pprint\n",
    "\n",
    "clean_dois = Clean_DOIs()\n",
    "data = list(csv.DictReader(lines[:10]))\n",
    "# To check if DOIs are valid\n",
    "checked_dois = clean_dois.check_dois_validity(data=data)\n",
    "# To clean DOIs\n",
    "output = clean_dois.procedure(data=checked_dois)\n",
    "\n",
    "pp.pprint(output)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[   {   'Already_valid': 0,\n        'Invalid_cited_DOI': '10.5555/646836.708343',\n        'Other-type_error': 0,\n        'Prefix_error': 0,\n        'Suffix_error': 0,\n        'Valid_DOI': '',\n        'Valid_citing_DOI': '10.14778/1920841.1920954'},\n    {   'Already_valid': 0,\n        'Invalid_cited_DOI': '10.2307/20184517',\n        'Other-type_error': 0,\n        'Prefix_error': 0,\n        'Suffix_error': 0,\n        'Valid_DOI': '',\n        'Valid_citing_DOI': '10.5406/ethnomusicology.59.2.0202'},\n    {   'Already_valid': 0,\n        'Invalid_cited_DOI': '10.1161/circ.37.4.509',\n        'Other-type_error': 0,\n        'Prefix_error': 0,\n        'Suffix_error': 0,\n        'Valid_DOI': '',\n        'Valid_citing_DOI': '10.1161/01.cir.63.6.1391'},\n    {   'Already_valid': 0,\n        'Invalid_cited_DOI': '10.3748/wjg.v10.i5.707.',\n        'Other-type_error': 0,\n        'Prefix_error': 0,\n        'Suffix_error': 1,\n        'Valid_DOI': '10.3748/WJG.V10.I5.707',\n        'Valid_citing_DOI': '10.1177/1179546820918903'},\n    {   'Already_valid': 0,\n        'Invalid_cited_DOI': '10.1070/10810730903528033',\n        'Other-type_error': 0,\n        'Prefix_error': 0,\n        'Suffix_error': 0,\n        'Valid_DOI': '',\n        'Valid_citing_DOI': '10.1080/10410236.2020.1731937'},\n    {   'Already_valid': 0,\n        'Invalid_cited_DOI': '10.1161/str.24.7.8322400',\n        'Other-type_error': 0,\n        'Prefix_error': 0,\n        'Suffix_error': 0,\n        'Valid_DOI': '',\n        'Valid_citing_DOI': '10.1161/strokeaha.112.652065'},\n    {   'Already_valid': 0,\n        'Invalid_cited_DOI': '10.1111/j.545-5300.2003.42208.x',\n        'Other-type_error': 0,\n        'Prefix_error': 0,\n        'Suffix_error': 0,\n        'Valid_DOI': '',\n        'Valid_citing_DOI': '10.1177/1049732310393747'},\n    {   'Already_valid': 0,\n        'Invalid_cited_DOI': '10.3760/cma.j.issn.0366-6999.20131202',\n        'Other-type_error': 0,\n        'Prefix_error': 0,\n        'Suffix_error': 0,\n        'Valid_DOI': '',\n        'Valid_citing_DOI': '10.1155/2017/1491405'},\n    {   'Already_valid': 0,\n        'Invalid_cited_DOI': '10.1161/res.35.2.159',\n        'Other-type_error': 0,\n        'Prefix_error': 0,\n        'Suffix_error': 0,\n        'Valid_DOI': '',\n        'Valid_citing_DOI': '10.1161/01.res.68.6.1549'}]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python392jvsc74a57bd0ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963",
   "display_name": "Python 3.9.2 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}