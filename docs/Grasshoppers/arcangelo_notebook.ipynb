{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratory Notebook"
   ]
  },
  {
   "source": [
    "## Preliminary note\n",
    "\n",
    "In order to correctly view the Javascript content and use the internal hyperlinks, please check the notebook at the following address: [https://nbviewer.jupyter.org/github/open-sci/2020-2021/blob/master/docs/Grasshoppers/arcangelo_notebook.ipynb](https://nbviewer.jupyter.org/github/open-sci/2020-2021/blob/master/docs/Grasshoppers/arcangelo_notebook.ipynb)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [24/03/2021 Preliminary research](#preliminary)\n",
    "2. [28/03/2021 Writing the abstract](#abstract)\n",
    "3. [29/03/2021 Full reading of the DOI Handbook](#handbook)\n",
    "4. [03/04/2021 Data Management Plan Version 1.0](#dmp1)\n",
    "5. [09/04/2021 Literature review](#literature)\n",
    "6. [11/04/2021 Computational workflow Version 1.0](#workflow1)\n",
    "7. [17/04/2021 Open peer review](#peerReview)\n",
    "8. [17/04/2021 First version of the regular expression to clean DOI's wrong prefixes](#regex1)\n",
    "9. [20/04/2021 Reflections on Open Source Licenses](#license)\n",
    "10. [20/04/2021 Code to handle HTTP requests](#requests)\n",
    "11. [22/04/2021 Improvements to the regex to clean up prefix errors and code modularization](#modularization)\n",
    "12. [23/04/2021 Final algorithm design](#final_algorithm)\n",
    "13. [27/04/2021-29/04/2021 Visualizations](#viz)\n",
    "14. [30/04/2021 Public Data File from Crossref](#crossref_dump)\n",
    "15. [01/05/2021-02/05/2021-03/05/2021 Writing and editing the article](#article)\n",
    "16. [08/05/2021 Results cache](#results_cache)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24/03/2021 Preliminary research <a name=\"preliminary\"></a>\n",
    "\n",
    "<p>Today I started working on the problem of wrong DOI names reported by Crossref. In particular, the research question on which the entire work focuses is which are the classes of errors that characterize the invalid DOI names and which classes can be addressed through automatic processes in order to obtain the correct DOI names.<p/>\n",
    "<p>Having never used the DOI API, I explored the subject by reading its documentation<sup><a href=\"#preliminary_ref_01\">[1]</a></sup>. In particular, in chapter <em>3.8.3 Proxy Server REST API</em> I discovered that by performing a GET to https://doi.org/api/handles/&lt;doi&gt; it is possible to obtain useful information in response. Among these, the status code is particularly interesting, which can take on four values:\n",
    "<ul>\n",
    "    <li>1: Success. (HTTP 200 OK)</li>\n",
    "    <li>2: Error. Something unexpected went wrong during handle resolution. (HTTP 500 Internal Server Error)</li>\n",
    "    <li>100: Handle Not Found. (HTTP 404 Not Found)</li>\n",
    "    <li>200: Values Not Found. The handle exists but has no values (or no values according to the types and indices specified). (HTTP 200 OK)</li>\n",
    "</ul>\n",
    "This is extremely useful for identifying those initially invalid DOI names that have become valid in the meantime.</p>\n",
    "   \n",
    "### Reference\n",
    "<ol>\n",
    "    <li id=\"preliminary_ref_01\">International DOI Foundation. (2019). DOIÂ® Handbook. <a href=\"https://doi.org/10.1000/182\" target=\"_blank\">https://doi.org/10.1000/182</a>.</li>\n",
    "</ol>"
   ]
  },
  {
   "source": [
    "## 28/03/2021 Writing the abstract <a name=\"abstract\"></a>\n",
    "\n",
    "<p>Together with my colleagues Cristian Santini (orcid: 0000-0001-7363-6737), Ricarda Boente (orcid: 0000-0002-2133-8735) and Deniz Tural (orcid: 0000-0002-6391-4198) I wrote the first version of the abstract.</p>\n",
    "<p>Starting from the initial hypothesis that there are two classes of errors, namely factual errors and DOI names that are not yet valid at the time of processing, we tried to further break down the first class into various subclasses: exploring the input dataset, we hypothesized that a DOI name may be factually wrong because it contains forbidden characters, because it contains excess strings at the beginning or at the end or due to a human error in the transcription.</p>\n",
    "<p>Moreover, I have elaborated a first hypotheses on how to deal with both errors' classes. As for the first class, I speculated that it would be possible to obtain the correct cited DOI names starting from the valid citing DOI names, using the REST API for COCI: to obtain through the COCI references operation all the cited articles and, using word embeddings algorithm, to identify the most similar DOI name to the wrong one. However, my colleagues have rightly pointed out that wrong DOI names are not accepted by COCI and that COCI is built from Crossref, so it would not be possible to get the correct DOI names from COCI. Another hypothesis proposed by Cristian was to exploit other metadata provided by Crossref relating to that particular wrong DOI name to check if there are other DOI names connected to it, which perhaps refer to the correct one. Finally, it was decided to remain as vague as possible in the abstract and to summarize the various hypotheses formulated in the expression \"rule-based methods\" with the idea of defining this point after further research.<p/>\n",
    "<p>Instead, there has been much more consensus on the strategy for dealing with the second class of errors, namely those due to DOI names that are not yet valid, that is, use the DOI API by interpreting the response status code.</p>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 29/03/2021 Full reading of the DOI Handbook <a name=\"handbook\"></a>\n",
    "\n",
    "<p>Writing the abstract raised some questions about the specifics of a DOI name. Therefore, I decided to read the entire documentation with the aim of answering three questions:\n",
    "<ol>\n",
    "    <li>Do DOI names use a limited character set?</li>\n",
    "    <li>Do DOI names have semantics?</li>\n",
    "    <li>In which ways can I query the DOI System Proxy Server?</li>\n",
    "</ol>\n",
    "<p>Chapter <em>2.5.1 Encoding principle</em> answers the first question:</p>\n",
    "<blockquote>\"DOI names may incorporate any printable characters from the Universal Character Set (UCS-2), of ISO / IEC 10646, which is the character set defined by Unicode. The character set encompasses most characters used in every major language written today. However, because of specific uses made of certain characters by some Internet technologies (the use of pointed brackets in xml for example), there may some effective restrictions in day-to-day use.\"</blockquote>\n",
    "<p>Chapter <em>2.2.1 General characteristics</em> answers the second question:</p>\n",
    "<blockquote>The DOI name is an opaque string for the purposes of the DOI system. No definitive information may be inferred from the specific character string of a DOI name. In particular, the inclusion in a DOI name of any registrant code allocated to a specific registrant does not provide evidence of the ownership of rights or current management responsibility of any intellectual property in the referent. Such information may be asserted in the associated metadata.</blockquote>\n",
    "<p>As for the third question, chapter <em>3.8.4.2 Parameter Passing</em> explores a series of queries that can be performed to specify the desired output. Therefore, I hypothesized that among the wrong DOI names there could be some that, during the extraction phase, had not been deprived of the query portion. The hypothesis was confirmed. The code used to perform this check is shown below.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The wrong DOI names that contain queries are 228 out of 1223297\nThe number of queries found is equal to 39. They are:\n{'?locale=', '?title=', '?accountid=', '?articleid=', '?term=', '?goto=', '?id=', '?slug=', '?src=', '?sequence=', '?nosfx=', '?select-row=', '?download=', '?code=', '?refreqid=', '?abstract_id=', '?rss=', '?ver=', '?crawler=', '?doi=', '?origin=', '?print=', '?error=', '?artid=', '?scroll=', '?site=', '?prd=', '?arnumber=', '?rskey=', '?page=', '?v=', '?no-access=', '?sid=', '?doid=', '?genre=', '?report=', '?uid=', '?ref=', '?seq='}\n"
     ]
    }
   ],
   "source": [
    "import csv, re, urllib.request\n",
    "\n",
    "url = 'https://zenodo.org/record/4625300/files/invalid_dois.csv'\n",
    "response = urllib.request.urlopen(url)\n",
    "lines = [l.decode('utf-8') for l in response.readlines()]\n",
    "reader = csv.reader(lines)\n",
    "rows_number = 0\n",
    "proxy_server_occurrences = list()\n",
    "proxy_server_queries = set()\n",
    "for row in reader:\n",
    "    rows_number += 1\n",
    "    query = re.search(\"\\?.+?=\", row[1])\n",
    "    if query is not None:\n",
    "        query = query.group(0)\n",
    "        proxy_server_occurrences.append(query)\n",
    "        proxy_server_queries.add(query)\n",
    "print(f\"The wrong DOI names that contain queries are {len(proxy_server_occurrences)} out of {rows_number}\")\n",
    "print(f\"The number of queries found is equal to {len(proxy_server_queries)}. They are:\")\n",
    "print(proxy_server_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03/04/2021 Data Management Plan Version 1.0 <a name=\"dmp1\"></a>\n",
    "\n",
    "<p>Together with my colleagues I compiled version 1.0 of the Data Management Plan, using the ARGOS platform by OpenAIRE and EUDAT. Two datasets were considered: one for the output and the other for the code. Particular care was taken in filling in as many fields as possible, but some doubts arose about which metadata to use in order to comply with the FAIR principles<sup><a href=\"#dmp1_ref_01\">[1]</a><a href=\"#dmp1_ref_02\">[2]</a><a href=\"#dmp1_ref_03\">[3]</a></sup>, an aspect that will be further explored later.</p>\n",
    "\n",
    "### Reference\n",
    "<ol>\n",
    "    <li id=\"dmp1_ref_01\">Wilkinson, M. D., Dumontier, M., Aalbersberg, I. J., Appleton, G., Axton, M., Baak, A., Blomberg, N., Boiten, J.-W., da Silva Santos, L. B., Bourne, P. E., Bouwman, J., Brookes, A. J., Clark, T., Crosas, M., Dillo, I., Dumon, O., Edmunds, S., Evelo, C. T., Finkers, R., â¦ Mons, B. (2016). The FAIR Guiding Principles for scientific data management and stewardship. Scientific Data, 3, 160018. https://doi.org/10.1038/sdata.2016.18.</li>\n",
    "    <li id=\"dmp1_ref_02\">GO FAIR. (2018). FAIR Principles. https://www.go-fair.org/fair-principles/.</li>\n",
    "    <li id=\"dmp1_ref_03\">Michener, W. K. (2015). Ten Simple Rules for Creating a Good Data Management Plan. PLOS Computational Biology, 11(10), e1004525. https://doi.org/10.1371/journal.pcbi.1004525.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 09/04/2021 Literature review <a name=\"literature\"></a>\n",
    "<p>Taking a cue from Cristian's laboratory notebook, I read the articles he reported on 09/04/2021 about past studies on our research topic. I found several interesting ideas in <em>Types of DOI errors of cited references in Web of Science with a cleaning method</em><sup><a href=\"#literature_ref_01\">[1]</a></sup> by Shuo Xu et al. :</p>\n",
    "<ul>\n",
    "    <li>The study suggests that there are three types of DOI errors, namely prefix-, suffix- and other-type errors. The other-type errors are further divided into three subgroups: (a) those containing special characters (such as 10.1034/j.1600-0404.2000.101004262x./), (b) incoherently described DOIs (such as 10.1038/), and (c) those with incomplete suffix but with correct DOI prefix (such as 10.1007/3-540-48194-X_).</li>\n",
    "    <li>Three regular expressions are proposed to clean up the respective types of errors.\n",
    "        <img src=\"https://media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs11192-019-03162-4/MediaObjects/11192_2019_3162_Fig2_HTML.png?as=webp\"/>\n",
    "It is worth mentioning that the aforementioned regular expressions are applied to strings already cleaned of double underscores, double periods, XML tags, spaces and forward slashes.\n",
    "    </li>\n",
    "    <li>Finally, the article mentions some problems that cannot be solved by this approach: (a) similar characters confused with each other, such as âOâ versus â0â, âbâ versus â6â and âOâ versus âQâ; (b) to distinguish the correct DOI name from multiple DOI names assigned to the same cited reference; (c) a DOI name assigned to some cited reference that cannot be resolved by the DOI system; (d) a DOI name that is resolvable, but points to some knowledge unit within the interested cited reference.</li>\n",
    "</ul>\n",
    "<p>Also, the article <em>DOI errors and possible solutions for Web of Science</em> by Juenwen Zhu et al.<sup><a href=\"#literature_ref_02\">[2]</a></sup> suggests that DOI names often contain the wrong characters for similarity to the right ones, such as \"O\" instead of \"0\", \"b\" instead of \"6\", \"O\" instead of \"Q\". A possible solution could then be to apply these replacements and verify the new DOI is resolved by the DOI System Proxy.</p>\n",
    "<p>Finally, the paper <em>Errors in DOI indexing by bibliometrics databases</em> by Fiorenzo Franceschini et al.<sup><a href=\"#literature_ref_03\">[3]</a></sup> reports a more generic classification of bibliographic database errors, i.e. it distinguishes between authors' errors in creating the list of cited resources, and database mapping errors, such as transcription errors. It is clear that our work focuses only on the second category. The article then proceeds with analyzing a further category of error, i.e. single DOI names associated with different papers. However, no solution is proposed to the problem, which is only highlighted.</p>\n",
    "\n",
    "## Reference\n",
    "<ol>\n",
    "    <li id=\"literature_ref_01\">Xu, S., Hao, L., An, X. et al. Types of DOI errors of cited references in Web of Science with a cleaning method. Scientometrics 120, 1427â1437 (2019). https://doi.org/10.1007/s11192-019-03162-4.</li>\n",
    "    <li id=\"literature_ref_02\">Zhu, J., Hu, G. &amp; Liu, W. DOI errors and possible solutions for Web of Science. Scientometrics 118, 709â718 (2019). https://doi.org/10.1007/s11192-018-2980-7.</li>\n",
    "    <li id=\"literature_ref_03\">Franceschini, F., Maisano, D., &amp; Mastrogiacomo, L. (2015). Errors in DOI indexing by bibliometric databases. Scientometrics, 102(3), 2181â2186. https://doi.org/10.1007/s11192-014-1503-4.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11/04/2021 Computational workflow Version 1.0 <a name=\"workflow1\"></a>\n",
    "\n",
    "<p>The first version of the computational workflow, that is the protocol, was carried out collectively by all members of the group from the beginning to the end.</p>\n",
    "<p>The platform chosen for creating, editing and publishing the protocol was Protocols.io (<a href=\"https://www.protocols.io/welcome\" alt=\"Protocols.io home page\" target=\"_blank\">https://www.protocols.io/welcome</a>).</p>\n",
    "<p>Having found valid ideas from the study of the existing literature, for the first version we decided to structure the protocol by reusing already existing methods, in particular as regards the classification of possible DOI errors (Buchanan, 2006) and as regards the cleaning of strings using regular expressions (Xu et al., 2019).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17/04/2021 Open peer review <a name=\"peerReview\"></a>\n",
    "<p>In order to better understand how to carry out and structure a peer review, I read the article <em>How to write a thorough peer review</em> (Column, 2018), which inspired me about the <em>modus operandi</em> as well as clarifying the spirit in which a review should be drafted. Specifically, having to review a computational workflow, I also deepened the <em>Guidelines for Reviewers</em> provided by PLOS ONE, which concern not only the protocols but in general the dynamics related to the review process.</p>\n",
    "<p>Having those premises, I tried to stick to both guidelines, taking three readings of the protocol, taking notes away and focusing on different aspects each time. I also tried to answer four questions:</p>\n",
    "<ul>\n",
    "    <li>Does the manuscript provide valid rationale for the planned or ongoing study, with clearly identified and justified research questions?</li>\n",
    "    <li>Is the protocol technically sound and planned in a manner that will lead to a meaningful outcome and allow testing of the stated hypotheses?</li>\n",
    "    <li>Have the authors described where all data underlying the findings will be made available when the study is complete?</li>\n",
    "    <li>Is the methodology feasible and does the description provide sufficient methodological detail for the protocol to be reproduced and replicated?</li>\n",
    "</ul>\n",
    "<p>Therefore, I organized the review in 4 chapters:</p>\n",
    "<ul>\n",
    "    <li><strong>The premises. About the study's rationale and impact</strong></li>\n",
    "    <li><strong>The methodology. About the protocol's technical soundness</strong></li>\n",
    "    <li><strong>The reproducibility. About the the input and output</strong></li>\n",
    "    <li><strong>Conclusions</strong></li>\n",
    "</ul>\n",
    "\n",
    "## Reference\n",
    "<ol>\n",
    "    <li>Stiller-Reeve, M. (2018). How to write a thorough peer review. Nature. https://doi.org/10.1038/d41586-018-06991-0.</li>\n",
    "    <li>Guidelines for Reviewers. PLOS ONE. https://journals.plos.org/plosone/s/reviewer-guidelines.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17/04/2021 First version of the regular expression to clean DOI's wrong prefixes <a name=\"regex1\"></a>\n",
    "<p>Trying to use the regular expression to clean up wrong DOI prefixes proposed in <em>Types of DOI errors of cited references in Web of Science with a cleaning method</em> (Zu, Shuo et al., 2019), I quickly realized that it would never could have matched. The regular expression is the following:</p>\n",
    "<p><code>^(?:D[0|O]I\\/?HTTP:\\/\\/DX.D[0|O]I.[0|O]RG\\/[0|O]RG\\/[:|\\/]\\\\\\\\d+\\\\\\\\.HTTP:\\/\\/DX.D[0|O]I.[0|O]RG\\/?)+(.*)</code></p>\n",
    "<p>The reasons why it cannot match are the following:</p>\n",
    "<ol>\n",
    "    <li>The various protocols and domain names are all mandatory, none optional, while it is more plausible that a match will occur if they are reported as optional with the exclamation mark.</li>\n",
    "    <li>The match does not necessarily occur at the beginning of the string. For example, in the following incorrect DOI taken from the input dataset, the protocol is in the middle of the string and not at the beginning: <code>10.2478/s11696-009-0027-5,10.1016/j.aca.2006.07.086.http://dx.doi.org/10.1016/j.aca.2006.07.086\"</code>.</li>\n",
    "</ol>\n",
    "<p>Therefore, I have written a new version of the regular expression, which takes these two aspects into account, and it is the following:</p>\n",
    "<p><code>(?:http:\\/\\/dx.d[0|o]i.[0|o]rg\\/)+(.*)</code></p>\n",
    "<p>It is expected to further refine it shortly.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20/04/2021 Reflections on Open Source Licenses <a name=\"license\"></a>\n",
    "\n",
    "<p>I have read several Open Source Licenses and, from what I understand, there are two types: the permissive ones (Apache License 2.0, BSD, MIT license, ISC) and the ones with the copyleft (GPL). Assuming that I had a hard time finding substantial differences between the various permissive licenses, I like the Apache License 2.0 more than the others, because the it is more explicit about what it means by every single term used, such as \"License\", \"Licensor\", \"Legal Entity\" and \" You \".</p>\n",
    "<p>However, in general, the GPL license seems to be the one that takes the concept of open-source more seriously, ensuring that every single redistribution of the source remains open-source. That way, even if a derivative work is distributed for a fee, by the time someone legally comes into possession of that product, the code would have to be released under the GPL. For this reason, I imagine the GPL is less popular than the other licenses, as it restricts the potential users of the software. However, this kind of limitation seems ethical to me, and since Science and Openness are the focus for this course and project, it seems that GPL would be a more relevant choice as a license.</p>\n",
    "\n",
    "## Reference\n",
    "\n",
    "1. Licenses & Standards. Open Source Iniziative. https://opensource.org/licenses.\n",
    "2. Open Source Initiative. (2007). The Open Source Definition. https://opensource.org/osd. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20/04/2021 Code to handle HTTP requests <a name=\"requests\"></a>\n",
    "\n",
    "Trying to check the validity of the over million DOIs present in the input dataset, my colleague Ricarda Boente realized that the execution times were extremely long, measurable in days, and that the execution often crashed before reaching the end.\n",
    "\n",
    "In order to avoid future crashes, I have implemented a helper function, handle_request, which takes care of handling all exceptions that might occur during an http request, such as server-side (500, 502, 504, 520, 521) and client-side errors (404). In case of server-side errors, the function waits for the response for 60 seconds, then makes a second attempt. If the second fails, cancel the request and save the error in a dictionary. This way, you can export a log file at the end of the run in case there were any errors. Moreover, all requests are saved in a cache file, so they never have to be made again in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\arcan\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages (2.22.0)\nRequirement already satisfied: requests_cache in c:\\users\\arcan\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages (0.6.0)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\arcan\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages (from requests) (1.25.6)\nRequirement already satisfied: certifi>=2017.4.17 in c:\\users\\arcan\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages (from requests) (2019.9.11)\nRequirement already satisfied: idna<2.9,>=2.5 in c:\\users\\arcan\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages (from requests) (2.8)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\arcan\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages (from requests) (3.0.4)\nRequirement already satisfied: itsdangerous in c:\\users\\arcan\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages (from requests_cache) (1.1.0)\nRequirement already satisfied: url-normalize>=1.4 in c:\\users\\arcan\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages (from requests_cache) (1.4.3)\nRequirement already satisfied: six in c:\\users\\arcan\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages (from url-normalize>=1.4->requests_cache) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests requests_cache\n",
    "\n",
    "import requests, requests_cache, json\n",
    "from requests import Session\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "class Support(object):\n",
    "    def _requests_retry_session(\n",
    "        tries=1,\n",
    "        status_forcelist=(500, 502, 504, 520, 521),\n",
    "        session=None\n",
    "    ) -> Session:\n",
    "        session = session or requests.Session()\n",
    "        retry = Retry(\n",
    "            total=tries,\n",
    "            read=tries,\n",
    "            connect=tries,\n",
    "            status_forcelist=status_forcelist,\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry)\n",
    "        session.mount('http://', adapter)\n",
    "        session.mount('https://', adapter)\n",
    "        return session\n",
    "\n",
    "    def handle_request(self, url:str, cache_path:str=\"\", error_log_dict:dict=dict()):\n",
    "        if cache_path != \"\":\n",
    "            requests_cache.install_cache(cache_path)\n",
    "        try:\n",
    "            data = self._requests_retry_session().get(url, timeout=60)\n",
    "            if data.status_code == 200:\n",
    "                return data.json()\n",
    "            else:\n",
    "                error_log_dict[url] = data.status_code\n",
    "        except Exception as e:\n",
    "            error_log_dict[url] = str(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22/04/2021 Improvements to the regex to clean up prefix errors and code modularization<a name=\"modularization\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I further refined the regular expression to capture prefix errors, in order to also capture the prefixes related to the new Proxy Server address, i.e. https://doi.org/, which was not considered by the regex taken from the aforementioned paper (Zu, Shuo et al., 2019). The refined regular expression is therefore:\n",
    "\n",
    "<p><code>\"(.*?)(?:\\.)?(?:http:\\/\\/dx\\.d[0|o]i\\.[0|o]rg\\/|https:\\/\\/doi\\.org\\/)(.*)\"</code></p>\n",
    "\n",
    "The new regex was able to catch over 22,000 errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The regular expression matched 22077 DOIs, for example:\n[   {   'Cleaned_prefix_DOI': '10.1016/j.aca.2006.07.086',\n        'Invalid_DOI': '10.1016/j.aca.2006.07.086.http://dx.doi.org/10.1016/j.aca.2006.07.086'},\n    {   'Cleaned_prefix_DOI': '10.1016/j.jlumin.2004.10.018',\n        'Invalid_DOI': '10.1016/j.jlumin.2004.10.018.http://dx.doi.org/10.1016/j.jlumin.2004.10.018'},\n    {   'Cleaned_prefix_DOI': '10.1016/0014-4894(72)90103-8',\n        'Invalid_DOI': '10.1016/0014-4894(72)90103-8.http://dx.doi.org/10.1016/0014-4894(72)90103-8'},\n    {   'Cleaned_prefix_DOI': '10.1002/app.23704',\n        'Invalid_DOI': '10.1002/app.23704.http://dx.doi.org/10.1002/app.23704'},\n    {   'Cleaned_prefix_DOI': '10.1006/jcat.2001.3474',\n        'Invalid_DOI': '10.1006/jcat.2001.3474.http://dx.doi.org/10.1006/jcat.2001.3474'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pprint\n",
    "\n",
    "def clean_prefixes(data:list, field:str) -> list:\n",
    "    cleaned_prefixes_dois = list()\n",
    "    prefixes_regex = \"(.*?)(?:\\.)?(?:http:\\/\\/dx\\.d[0|o]i\\.[0|o]rg\\/|https:\\/\\/doi\\.org\\/)(.*)\"\n",
    "    for row in data:\n",
    "        invalid_doi = row[field]\n",
    "        match = re.search(prefixes_regex, invalid_doi, re.IGNORECASE)\n",
    "        if match:\n",
    "            new_doi = match.group(1) if match.group(1) > match.group(2) else match.group(2)\n",
    "            cleaned_prefix_doi = {\"Invalid_DOI\": row[\"Invalid_cited_DOI\"], \"Cleaned_prefix_DOI\": new_doi}\n",
    "            cleaned_prefixes_dois.append(cleaned_prefix_doi)\n",
    "    return cleaned_prefixes_dois\n",
    "\n",
    "import csv, re, urllib.request\n",
    "\n",
    "reader = csv.DictReader(lines)\n",
    "cleaned_prefixes_dois = clean_prefixes(data=reader, field=\"Invalid_cited_DOI\")\n",
    "print(f\"The regular expression matched {len(cleaned_prefixes_dois)} DOIs, for example:\")\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(cleaned_prefixes_dois[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also reorganized the code, in order to make it more modular, reusable and maintainable. Therefore, I defined a new Clean_DOIs class, which contains all the methods to validate and clean DOIs, and a second Support class, which takes care of handling inputs, outputs and requests."
   ]
  },
  {
   "source": [
    "## 23/04/2021 Final algorithm design<a name=\"final_algorithm\"></a>\n",
    "\n",
    "<p>I reorganized all the code written by my colleagues Cristian, Ricarda and Deniz into a single final algorithm, divided into three methods of the Clean_DOIs class, namely <strong>check_dois_validity</strong>, <strong>procedure</strong> and <strong>clean_doi</strong>.</p> \n",
    "<ul>\n",
    "    <li>The check_dois_validity method verifies how many DOIs of the input dataset belong   to the class of previously invalid DOIs that have become valid, through requests to the DOI Proxy Server, and outputs a list of dictionaries, each consisting of 4 keys: \"Valid_citing_DOI\", which contains the valid citing DOI in input, \"Invalid_cited_DOI\", which contains the invalid cited DOI in input, \"Valid_DOI\", which contains the valid DOI if the invalid DOI is actually valid or an empty string otherwise, and \"Already_valid\", which is equal to one if the previous case is true, 0 if it is false.</li>\n",
    "    <li>The clean_doi method applies the regular expressions to clean prefixes and suffixes and corrects the other-type errors sequentially, saving which class of error was found in a dictionary. This dictionary is eventually returned along with the correct DOI.</li>\n",
    "    <li>Finally, the procedure method takes care of executing the clean_doi method on each row of the input dictionaries list and verifying if the new DOI is valid through the DOI Proxy Server, finally returning the actual output, consisting of a list of 7-key dictionaries: \"Valid_citing_DOI\", \"Invalid_cited_DOI\", \"Valid_DOI\" and \"Already_valid\", taken from the output of the check_dois_validity method, and \"Prefix_error\", \"Suffix_error\" and \"Other-type_error\", which correspond to 1 if the found error belongs to that class, 0 otherwise.</li>\n",
    "</ul>\n",
    "<p>It is worth noting that the check_dois_validity and procedure methods both cycle over the input dataset in an apparently inefficient way because the entire workflow could be completed in one cycle. However, the disadvantages of carrying out the entire process in one cycle are greater than the advantages, for two reasons:</p>\n",
    "<ul>\n",
    "    <li>The procedure, being applied to more than one million DOIs, takes a long time. Dividing it into two steps allows to save the results of the first phase and start from the second without having to repeat everything from the beginning.</li>\n",
    "    <li>The second phase does not repeat the requests to the DOI Proxy Server made by the first, but makes new ones only if the first phase has returned 0 to the \"Already_valid\" field and if the clean DOI is different from the invalid one. Therefore, the advantages of dividing the process are greater than the disadvantages.</li>\n",
    "</ul>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class Clean_DOIs(object):\n",
    "    def __init__(self, cache_path:str=\"\", logs:dict={}):\n",
    "        self.cache_path = cache_path\n",
    "        self.logs = logs\n",
    "        self.prefix_regex = \"(.*?)(?:\\.)?(?:HTTP:\\/\\/DX\\.D[0|O]I\\.[0|O]RG\\/|HTTPS:\\/\\/D[0|O]I\\.[0|O]RG\\/)(.*)\"\n",
    "        suffix_dcsupplemental = \"\\/-\\/DCSUPPLEMENTAL\"\n",
    "        suffix_suppinfo = \"SUPPINF[0|O](\\.)?\"\n",
    "        suffix_pmid1 = \"[\\.|\\(|,|;]?PMID:\\d+.*?\"\n",
    "        suffix_pmid2 = \"[\\.|\\(|,|;]?PMCID:PMC\\d+.*?\"\n",
    "        suffix_epub = \"[\\(|\\[]EPUBAHEADOFPRINT[\\)\\]]\"\n",
    "        suffix_published_online = \"[\\.|\\(|,|;]?ARTICLEPUBLISHEDONLINE.*?\\d{4}\"\n",
    "        suffix_http = \"[\\.|\\(|,|;]*HTTP:\\/\\/.*?\"\n",
    "        suffix_subcontent = \"\\/(META|ABSTRACT|FULL|EPDF|PDF|SUMMARY)([>|\\)](LAST)?ACCESSED\\d+)?\"\n",
    "        suffix_accessed = \"[>|\\)](LAST)?ACCESSED\\d+\"\n",
    "        suffix_sagepub = \"[\\.|\\(|,|;]?[A-Z]*\\.?SAGEPUB.*?\"\n",
    "        suffix_dotted_line = \"\\.{5}.*?\"\n",
    "        suffix_delimiters = \"[\\.|,|<|&|\\(|;]+\"\n",
    "        suffix_double_doi = \"[\\.|\\(|,|;]10.\\d{4}\\/.*?\"\n",
    "        suffix_doi_mark = \"\\[DOI\\].*?\"\n",
    "        suffix_year = \"\\(\\d{4}\\)?\"\n",
    "        suffix_query = \"\\?.*?=.*?\"\n",
    "        suffix_hash = \"#.*?\"\n",
    "        suffix_regex_lst = [suffix_dcsupplemental, suffix_suppinfo, suffix_pmid1, suffix_pmid2, suffix_epub,\n",
    "                            suffix_published_online, suffix_http, suffix_subcontent, suffix_accessed, suffix_sagepub,\n",
    "                            suffix_dotted_line, suffix_delimiters, suffix_double_doi, suffix_doi_mark, suffix_year,\n",
    "                            suffix_query, suffix_hash]\n",
    "        self.suffix_regex = \"(.*?)(?:\" + \"|\".join(suffix_regex_lst) + \")$\"\n",
    "\n",
    "    def check_dois_validity(self, data:list) -> list:\n",
    "        checked_dois = list()\n",
    "        for row in data:\n",
    "            invalid_dictionary = {\n",
    "                \"Valid_citing_DOI\": row[\"Valid_citing_DOI\"],\n",
    "                \"Invalid_cited_DOI\": row[\"Invalid_cited_DOI\"], \n",
    "                \"Valid_DOI\": \"\",\n",
    "                \"Already_valid\": 0\n",
    "            }\n",
    "            handle = Support().handle_request(url=f\"https://doi.org/api/handles/{row['Invalid_cited_DOI']}\", cache_path=self.cache_path, error_log_dict=self.logs)\n",
    "            if handle is not None:\n",
    "                if handle[\"responseCode\"] == 1:\n",
    "                    checked_dois.append(\n",
    "                        {\"Valid_citing_DOI\": row[\"Valid_citing_DOI\"],\n",
    "                        \"Invalid_cited_DOI\": row[\"Invalid_cited_DOI\"], \n",
    "                        \"Valid_DOI\": row['Invalid_cited_DOI'],\n",
    "                        \"Already_valid\": 1\n",
    "                        })\n",
    "                else:\n",
    "                    checked_dois.append(invalid_dictionary)       \n",
    "            else:\n",
    "                checked_dois.append(invalid_dictionary)             \n",
    "        return checked_dois\n",
    "    \n",
    "    def procedure(self, data:list) -> list:\n",
    "        output = list()\n",
    "        for row in data:\n",
    "            invalid_doi = row[\"Invalid_cited_DOI\"]\n",
    "            invalid_dictionary = {\n",
    "                \"Valid_citing_DOI\": row[\"Valid_citing_DOI\"],\n",
    "                \"Invalid_cited_DOI\": row[\"Invalid_cited_DOI\"],\n",
    "                \"Valid_DOI\": row[\"Valid_DOI\"],\n",
    "                \"Already_valid\": row[\"Already_valid\"],\n",
    "                \"Prefix_error\": 0,\n",
    "                \"Suffix_error\": 0,\n",
    "                \"Other-type_error\": 0\n",
    "            }\n",
    "            if row[\"Already_valid\"] != 1:\n",
    "                new_doi, classes_of_errors = self.clean_doi(row[\"Invalid_cited_DOI\"])\n",
    "                valid_dictionary = {\n",
    "                    \"Valid_citing_DOI\": row[\"Valid_citing_DOI\"],\n",
    "                    \"Invalid_cited_DOI\": row[\"Invalid_cited_DOI\"],\n",
    "                    \"Valid_DOI\": new_doi,\n",
    "                    \"Already_valid\": row[\"Already_valid\"],\n",
    "                    \"Prefix_error\": classes_of_errors[\"prefix\"],\n",
    "                    \"Suffix_error\": classes_of_errors[\"suffix\"],\n",
    "                    \"Other-type_error\": classes_of_errors[\"other-type\"]\n",
    "                }\n",
    "                if new_doi != row[\"Invalid_cited_DOI\"]:\n",
    "                    handle = Support().handle_request(url=f\"https://doi.org/api/handles/{new_doi}\", cache_path=self.cache_path, error_log_dict=self.logs)\n",
    "                    if handle is not None:\n",
    "                        output.append(valid_dictionary)\n",
    "                    else:\n",
    "                        output.append(invalid_dictionary)\n",
    "                else:\n",
    "                    output.append(invalid_dictionary)\n",
    "            else:\n",
    "                output.append(invalid_dictionary)\n",
    "        return output\n",
    "    \n",
    "    def clean_doi(self, doi:str) -> str:\n",
    "        tmp_doi = doi.upper().replace(\" \", \"\")\n",
    "        prefix_match = re.search(self.prefix_regex, tmp_doi)\n",
    "        classes_of_errors = {\n",
    "            \"prefix\": 0,\n",
    "            \"suffix\": 0,\n",
    "            \"other-type\": 0\n",
    "        }\n",
    "        if prefix_match:\n",
    "            tmp_doi = prefix_match.group(1)\n",
    "            classes_of_errors[\"prefix\"] += 1\n",
    "        suffix_match = re.search(self.suffix_regex, tmp_doi)\n",
    "        if suffix_match:\n",
    "            tmp_doi = suffix_match.group(1)\n",
    "            classes_of_errors[\"suffix\"] += 1\n",
    "        new_doi = re.sub(\"\\\\\\\\\", \"\", tmp_doi)\n",
    "        new_doi = re.sub(\"__\", \"_\", tmp_doi)\n",
    "        new_doi = re.sub(\"\\\\.\\\\.\", \".\", tmp_doi)\n",
    "        new_doi = re.sub(\"<.*?>.*?</.*?>\", \"\", tmp_doi)\n",
    "        new_doi = re.sub(\"<.*?/>\", \"\", tmp_doi)\n",
    "        if new_doi != tmp_doi:\n",
    "            classes_of_errors[\"other-type\"] += 1\n",
    "        return new_doi, classes_of_errors\n"
   ]
  },
  {
   "source": [
    "Finally, I created a tutorial.py program, which illustrates through comments how to perform the entire procedure from the beginning to the end and executes it, for the benefit of reproducibility. The current version can be found at the following address: https://github.com/open-sci/2020-2021-grasshoppers-code/blob/e50e52f728f442cea35bdf72641e644f02d88fea/tutorial.py. It is not possible to run it similarly on Jupyter Notebook, but I still want to show a sample of the final output."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import pprint\n",
    "\n",
    "clean_dois = Clean_DOIs()\n",
    "data = list(csv.DictReader(lines[:10]))\n",
    "# To check if DOIs are valid\n",
    "checked_dois = clean_dois.check_dois_validity(data=data)\n",
    "# To clean DOIs\n",
    "output = clean_dois.procedure(data=checked_dois)\n",
    "\n",
    "pp.pprint(output)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[   {   'Already_valid': 0,\n        'Invalid_cited_DOI': '10.5555/646836.708343',\n        'Other-type_error': 0,\n        'Prefix_error': 0,\n        'Suffix_error': 0,\n        'Valid_DOI': '',\n        'Valid_citing_DOI': '10.14778/1920841.1920954'},\n    {   'Already_valid': 0,\n        'Invalid_cited_DOI': '10.2307/20184517',\n        'Other-type_error': 0,\n        'Prefix_error': 0,\n        'Suffix_error': 0,\n        'Valid_DOI': '',\n        'Valid_citing_DOI': '10.5406/ethnomusicology.59.2.0202'},\n    {   'Already_valid': 0,\n        'Invalid_cited_DOI': '10.1161/circ.37.4.509',\n        'Other-type_error': 0,\n        'Prefix_error': 0,\n        'Suffix_error': 0,\n        'Valid_DOI': '',\n        'Valid_citing_DOI': '10.1161/01.cir.63.6.1391'},\n    {   'Already_valid': 0,\n        'Invalid_cited_DOI': '10.3748/wjg.v10.i5.707.',\n        'Other-type_error': 0,\n        'Prefix_error': 0,\n        'Suffix_error': 1,\n        'Valid_DOI': '10.3748/WJG.V10.I5.707',\n        'Valid_citing_DOI': '10.1177/1179546820918903'},\n    {   'Already_valid': 0,\n        'Invalid_cited_DOI': '10.1070/10810730903528033',\n        'Other-type_error': 0,\n        'Prefix_error': 0,\n        'Suffix_error': 0,\n        'Valid_DOI': '',\n        'Valid_citing_DOI': '10.1080/10410236.2020.1731937'},\n    {   'Already_valid': 0,\n        'Invalid_cited_DOI': '10.1161/str.24.7.8322400',\n        'Other-type_error': 0,\n        'Prefix_error': 0,\n        'Suffix_error': 0,\n        'Valid_DOI': '',\n        'Valid_citing_DOI': '10.1161/strokeaha.112.652065'},\n    {   'Already_valid': 0,\n        'Invalid_cited_DOI': '10.1111/j.545-5300.2003.42208.x',\n        'Other-type_error': 0,\n        'Prefix_error': 0,\n        'Suffix_error': 0,\n        'Valid_DOI': '',\n        'Valid_citing_DOI': '10.1177/1049732310393747'},\n    {   'Already_valid': 0,\n        'Invalid_cited_DOI': '10.3760/cma.j.issn.0366-6999.20131202',\n        'Other-type_error': 0,\n        'Prefix_error': 0,\n        'Suffix_error': 0,\n        'Valid_DOI': '',\n        'Valid_citing_DOI': '10.1155/2017/1491405'},\n    {   'Already_valid': 0,\n        'Invalid_cited_DOI': '10.1161/res.35.2.159',\n        'Other-type_error': 0,\n        'Prefix_error': 0,\n        'Suffix_error': 0,\n        'Valid_DOI': '',\n        'Valid_citing_DOI': '10.1161/01.res.68.6.1549'}]\n"
     ]
    }
   ]
  },
  {
   "source": [
    "## 27/04/2021-29/04/2021 Visualizations<a name=\"viz\"></a>\n",
    "\n",
    "In order to effectively show the results obtained, I made two visualizations using [d3.js](https://d3js.org/).\n",
    "\n",
    "The **barplot** helps to compare the values through the difference in length of the associated bars. The values considered were the number of DOIs still invalid after the algorithm was run, the number of valid ones and the number of DOIs associated with each considered error class, i.e. DOIs already valid or containing prefix, suffix and other-type errors.\n",
    "\n",
    "On the other hand, the **treemap** helps to read the data from a hierarchical point of view and to compare the values through area differences. In this case, the root is made up of the set of all DOIs, which inside includes the DOIs still invalid after the algorithm has been run and those that have been corrected. This last category can be further subdivided into DOIs that are valid because they were already valid or because they contained prefix, suffix or other-type errors that have been corrected.\n",
    "\n",
    "The visualizations are interactive: it is possible to reorder the barplot values in ascending or descending order and, by hovering the mouse over the graphs, a tooltip is shown that indicates the current measure, the number of DOIs and the percentage of the total.\n",
    "\n",
    "### Reference\n",
    "- Bostock, M. (2021). d3.js. https://archive.softwareheritage.org/swh:1:dir:35fe697ae5a21e96d9fc01d890b30010e23c16dd\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x6dcf670>"
      ],
      "text/html": "\n        <iframe\n            width=\"100%\"\n            height=\"1000\"\n            src=\"https://open-sci.github.io/2020-2021-grasshoppers-code/\"\n            frameborder=\"0\"\n            allowfullscreen\n        ></iframe>\n        "
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src='https://open-sci.github.io/2020-2021-grasshoppers-code/', width=\"100%\", height=1000)"
   ]
  },
  {
   "source": [
    "## 30/04/2021 Public Data File from Crossref <a name=\"crossref_dump\"></a>\n",
    "\n",
    "<p>The main difficulty of this research is certainly in the management of the large amount of requests to the DOI Proxy Server, which is complex both in terms of execution stability and in terms of the time it is necessary to wait before obtaining a result. If a request is made per second (which is not the case, because many DOIs are wrong and the timeout has been set to 10 seconds), it takes 1'200'000 seconds to process 1'200'000 DOIs, which corresponds to 20'000 minutes, or 13 days and 21 hours. The cache doesn't help reduce the time from the second run onwards either, because the cache only saves the correct results, the ones that received an HTTP 200 OK response, while the 404s, which are the most, need to be repeated.</p>\n",
    "<p>However, there is a way to dramatically reduce the execution time, which is to use the Crossref dump. The DOIs present at the \"work\" level in Crossref are assigned by Crossref itself and, therefore, are necessarily correct, unlike those at the \"reference\" level, which are instead assigned by publishers and which this work intends to verify and correct.</p>\n",
    "\n",
    "Therefore, I downloaded all Crossref via BitTorrent from the following address <a href=\"https://doi.org/10.13003/GU3DQMJVG4\" target=\"_blank\" alt=\"Crossref dump link\">https://doi.org/10.13003/GU3DQMJVG4</a>, which took about 8 hours. The uncompressed file weighs over half a TB and consists of over 40,000 json files. Then, I wrote an algorithm to parse them all and save only the DOIs at work level in a csv file. However, I was unable to process all of Crossref, as the process is clearly very demanding in terms of RAM, and I was able to generate a CSV file corresponding to only half of Crossref. Note: The computer that was used has 32 GB of RAM. In order to make this process more efficient, I chose to use the ijson library,which instead of loading each json file, reads them as a stream, responding only to specific events, similar to what SAX does for XML.\n",
    "\n",
    "The output CSV file was later used to generate a set containing all DOIs. The set and not the list was chosen because the complexity of the control operation if an element is contained within it is O(1), while for a list it is O(n). In other words, for a set it is instantaneous, while for a list the time required is directly proportional to the number of elements in the list, which in this case is millions of DOIs. The reason is that the elements in a set are hashed, which requires even more RAM.\n",
    "\n",
    "After that, the algorithm was modified to check if each DOI is present in the set before making a request to the DOI Proxy Server. A substantial improvement in speed was observed. However, the occupied RAM was 21 GB. Therefore, I decided to make the import of DOIs from Crossref as an optional step, leaving the user the possibility to choose whether to favor execution times or RAM.\n",
    "\n",
    "### Reference\n",
    "\n",
    "- Crossref. (2021). January 2021 Public Data File from Crossref. https://doi.org/10.13003/GU3DQMJVG4 \n",
    "- The International Centre for Radio Astronomy Research (2021). IJson. https://archive.softwareheritage.org/swh:1:dir:fe3149dc1f824f783d0783191a65917816a0404b\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: ijson in c:\\users\\arcan\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages (3.1.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install ijson\n",
    "import ijson, os\n",
    "\n",
    "def get_all_crossref_dois(folder_path:str=\"./dataset/crossref/\"):\n",
    "    json_files = [pos_json for pos_json in os.listdir(folder_path) if pos_json.endswith('.json')]\n",
    "    dois = list()\n",
    "    pbar = tqdm(total=len(json_files))\n",
    "    for json_file in json_files:\n",
    "        with open(os.path.join(folder_path, json_file)) as json_file:\n",
    "            parser = ijson.parse(json_file)\n",
    "            for prefix, event, value in parser:\n",
    "                if (prefix, event) == ('items.item.DOI', 'string'):\n",
    "                    dois.append({\"crossref_doi\": value})\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    return dois"
   ]
  },
  {
   "source": [
    "## 01/05/2021-02/05/2021-03/05/2021 Writing and editing the article <a name=\"article\"></a>\n",
    "\n",
    "<p>In this three days I focused on writing the article and, in particular, on the chapter about the literature review, on the parts dedicated to regular expressions to clean prefixes and on the use of the <em>Public Data File from Crossref</em>. In addition, I have particularly taken care of the final revision of the article and the correction of language errors.</p>\n",
    "<p>Moreover, I reviewed the protocol, the DMP, the code, the output dataset and published the final version of all these materials on Zenodo, after which I included the respective DOIs in the <strong>material.md</strong> file published on the course repository.</p>\n",
    "<p>Finally, since Cristian had indicated the number of occurrences for each of his patterns in the paper, I decided to make these numbers official by creating an algorithm that would count them all, and including it in the Clean_DOIs() class. Its execution can be observed in the following blocks.</p>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clean_DOIs(object):\n",
    "    def __init__(self, crossref_dois:list=list(), cache_path:str=\"\", logs:dict={}):\n",
    "        self.crossref_dois = crossref_dois\n",
    "        if len(self.crossref_dois) > 0:\n",
    "            print(\"[Clean_DOIs: INFO] Storing Crossref DOIs in a set\")\n",
    "            self.crossref_dois = {item[\"crossref_doi\"] for item in crossref_dois}\n",
    "        self.cache_path = cache_path\n",
    "        self.logs = logs\n",
    "        prefix_dx = \"HTTP:\\/\\/DX\\.D[0|O]I\\.[0|O]RG\\/\"\n",
    "        prefix_doi = \"HTTPS:\\/\\/D[0|O]I\\.[0|O]RG\\/\"\n",
    "        suffix_dcsupplemental = \"\\/-\\/DCSUPPLEMENTAL\"\n",
    "        suffix_suppinfo = \"SUPPINF[0|O](\\.)?\"\n",
    "        suffix_pmid1 = \"[\\.|\\(|,|;]?PMID:\\d+.*?\"\n",
    "        suffix_pmid2 = \"[\\.|\\(|,|;]?PMCID:PMC\\d+.*?\"\n",
    "        suffix_epub = \"[\\(|\\[]EPUBAHEADOFPRINT[\\)\\]]\"\n",
    "        suffix_published_online = \"[\\.|\\(|,|;]?ARTICLEPUBLISHEDONLINE.*?\\d{4}\"\n",
    "        suffix_http = \"[\\.|\\(|,|;]*HTTP:\\/\\/.*?\"\n",
    "        suffix_subcontent = \"\\/(META|ABSTRACT|FULL|EPDF|PDF|SUMMARY)([>|\\)](LAST)?ACCESSED\\d+)?\"\n",
    "        suffix_accessed = \"[>|\\)](LAST)?ACCESSED\\d+\"\n",
    "        suffix_sagepub = \"[\\.|\\(|,|;]?[A-Z]*\\.?SAGEPUB.*?\"\n",
    "        suffix_dotted_line = \"\\.{5}.*?\"\n",
    "        suffix_delimiters = \"[\\.|,|<|&|\\(|;]+\"\n",
    "        suffix_doi_mark = \"\\[DOI\\].*?\"\n",
    "        suffix_year = \"\\(\\d{4}\\)?\"\n",
    "        suffix_query = \"\\?.*?=.*?\"\n",
    "        suffix_hash = \"#.*?\"\n",
    "        self.suffix_regex_lst = [suffix_dcsupplemental, suffix_suppinfo, suffix_pmid1, suffix_pmid2, suffix_epub,\n",
    "                            suffix_published_online, suffix_http, suffix_subcontent, suffix_accessed, suffix_sagepub,\n",
    "                            suffix_dotted_line, suffix_delimiters, suffix_doi_mark, suffix_year,\n",
    "                            suffix_query, suffix_hash]\n",
    "        self.prefix_regex_lst = [prefix_dx, prefix_doi]\n",
    "        self.prefix_regex = \"(.*?)(?:\\.)?(?:\" + \"|\".join(self.prefix_regex_lst) + \")(.*)\"\n",
    "        self.suffix_regex = \"(.*?)(?:\" + \"|\".join(self.suffix_regex_lst) + \")$\"\n",
    "    \n",
    "    def get_number_of_matches(self, data:list) -> dict:\n",
    "        classes_of_errors = {\n",
    "            \"other-type\": {\n",
    "                \"\\\\\\\\\": 0,\n",
    "                \"__\": 0,\n",
    "                \"\\\\.\\\\.\": 0,\n",
    "                \"<.*?>.*?</.*?>\": 0,\n",
    "                \"<.*?/>\": 0\n",
    "            }\n",
    "        }\n",
    "        for regex in self.prefix_regex_lst:\n",
    "            classes_of_errors.setdefault(\"prefix\", dict())\n",
    "            classes_of_errors[\"prefix\"].setdefault(regex, 0)\n",
    "        for regex in self.suffix_regex_lst:\n",
    "            classes_of_errors.setdefault(\"suffix\", dict())\n",
    "            classes_of_errors[\"suffix\"].setdefault(regex, 0)\n",
    "        for category in classes_of_errors:\n",
    "            for regex in classes_of_errors[category]:\n",
    "                for row in data:\n",
    "                    doi = row[\"Invalid_cited_DOI\"].upper()\n",
    "                    match = re.search(regex, doi)\n",
    "                    if match:\n",
    "                        classes_of_errors[category][regex] += 1\n",
    "        return classes_of_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{   'other-type': {   '<.*?/>': 40,\n                      '<.*?>.*?</.*?>': 2,\n                      '\\\\.\\\\.': 1416,\n                      '\\\\\\\\': 1030,\n                      '__': 38},\n    'prefix': {   'HTTP:\\\\/\\\\/DX\\\\.D[0|O]I\\\\.[0|O]RG\\\\/': 21983,\n                  'HTTPS:\\\\/\\\\/D[0|O]I\\\\.[0|O]RG\\\\/': 94},\n    'suffix': {   '#.*?': 538,\n                  'SUPPINF[0|O](\\\\.)?': 50,\n                  '[>|\\\\)](LAST)?ACCESSED\\\\d+': 67,\n                  '[\\\\(|\\\\[]EPUBAHEADOFPRINT[\\\\)\\\\]]': 5,\n                  '[\\\\.|,|<|&|\\\\(|;]+': 1223296,\n                  '[\\\\.|\\\\(|,|;]*HTTP:\\\\/\\\\/.*?': 22197,\n                  '[\\\\.|\\\\(|,|;]?ARTICLEPUBLISHEDONLINE.*?\\\\d{4}': 8,\n                  '[\\\\.|\\\\(|,|;]?PMCID:PMC\\\\d+.*?': 0,\n                  '[\\\\.|\\\\(|,|;]?PMID:\\\\d+.*?': 81,\n                  '[\\\\.|\\\\(|,|;]?[A-Z]*\\\\.?SAGEPUB.*?': 66,\n                  '\\\\(\\\\d{4}\\\\)?': 14165,\n                  '\\\\.{5}.*?': 565,\n                  '\\\\/(META|ABSTRACT|FULL|EPDF|PDF|SUMMARY)([>|\\\\)](LAST)?ACCESSED\\\\d+)?': 3155,\n                  '\\\\/-\\\\/DCSUPPLEMENTAL': 15,\n                  '\\\\?.*?=.*?': 228,\n                  '\\\\[DOI\\\\].*?': 94}}\n"
     ]
    }
   ],
   "source": [
    "data = list(csv.DictReader(lines))\n",
    "clean_dois = Clean_DOIs()\n",
    "number_of_matches = clean_dois.get_number_of_matches(data=data)\n",
    "pp.pprint(number_of_matches)"
   ]
  },
  {
   "source": [
    "## 08/05/2021 Results cache <a name=\"results_cache\"></a>\n",
    "\n",
    "During the presentation of the results on 05/05/2021, our work received some useful criticism. Among those is the following:\n",
    "\n",
    "<blockquote>You said that it is pretty impossible that the process stop to compute the various values. But what does it happen if someone by accident switch off your computer while running the computation? Do you have a way to restart it from the last data produced, avoiding to start it again from the beginning?</blockquote>\n",
    "\n",
    "Indeed it is true, if the PC were to shut down the entire work (except for the cache of requests to the DOI Proxy server) would be lost. In order to deal with this problem, I have implemented a new support method, <strong>read_cache</strong>, which if a cache file has been created, reads the data processed up to that moment, in order to restart from the last CSV line read and not from the beginning. This function has been integrated in both check_dois_validity and procedures, that create or update a cache file every n DOIs. The number of DOIs after which to update the cache is customizable, as is the location of the cache file.\n",
    "\n",
    "The code was inspired by the strategy adopted by the Leftovers 2.0 team to solve the same problem (Coppini, S. et al., 2021), and can be viewed in the next block.\n",
    "\n",
    "Finally, I created a new version of the protocol and updated it to account for the new code.\n",
    "\n",
    "### Reference\n",
    "Sara Coppini, Arianna Moretti, Alessia Cioffi, Nooshin Shahidzadeh Asadi, & Silvio Peroni. (2021, May 3). open-sci/2020-2021-the-leftovers-20-code: Investigating Missing Citations in COCI (Version v1.0.0). Zenodo. http://doi.org/10.5281/zenodo.4735621"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def read_cache(cache_path:str) -> (int, list):\n",
    "    num = 0\n",
    "    data = list()\n",
    "    if not os.path.exists(cache_path):\n",
    "        return num, data\n",
    "    with open(cache_path, 'r', encoding='utf8') as read_obj:\n",
    "        dict_reader = csv.DictReader(read_obj)\n",
    "        for row in dict_reader:\n",
    "            row_data = {\n",
    "                \"Valid_citing_DOI\": row[\"Valid_citing_DOI\"],\n",
    "                \"Invalid_cited_DOI\": row[\"Invalid_cited_DOI\"],\n",
    "                \"Valid_DOI\": row[\"Valid_DOI\"],\n",
    "                \"Already_valid\": row[\"Already_valid\"],\n",
    "                \"Prefix_error\": row.get(\"Prefix_error\"),\n",
    "                \"Suffix_error\": row.get(\"Suffix_error\"),\n",
    "                \"Other-type_error\": row.get(\"Other-type_error\")\n",
    "            }\n",
    "            data.append(row_data)\n",
    "            num += 1\n",
    "        return num, data\n",
    "\n",
    "def check_dois_validity(self, data:list, cache_path:str, cache_every:int=100) -> list:\n",
    "    # Get the number of the last processed csv line and the data obtained so far\n",
    "    start_index, checked_dois = Support.read_cache(cache_path=cache_path)\n",
    "    pbar = tqdm(total=len(data)-start_index)\n",
    "    # The data to consider starts from where it left off\n",
    "    data = islice(data, start_index + 1, None)\n",
    "    i = 0\n",
    "    for row in data:\n",
    "        # every n DOIs dump the data in a cache file (every 100 by default)\n",
    "        if i == cache_every:\n",
    "            Support.dump_csv(data=checked_dois, path=cache_path)\n",
    "            i = 0\n",
    "        valid_citing_doi = row[\"Valid_citing_DOI\"]\n",
    "        invalid_cited_doi = row[\"Invalid_cited_DOI\"]\n",
    "        invalid_dictionary = {\n",
    "            \"Valid_citing_DOI\": valid_citing_doi,\n",
    "            \"Invalid_cited_DOI\": invalid_cited_doi, \n",
    "            \"Valid_DOI\": \"\",\n",
    "            \"Already_valid\": 0\n",
    "        }\n",
    "        if invalid_cited_doi in self.crossref_dois:\n",
    "            handle = {\"responseCode\": 1}\n",
    "        else:\n",
    "            handle = Support().handle_request(url=f\"https://doi.org/api/handles/{invalid_cited_doi}\", cache_path=self.request_cache, error_log_dict=self.logs)\n",
    "        if handle is not None:\n",
    "            if handle[\"responseCode\"] == 1:\n",
    "                checked_dois.append(\n",
    "                    {\"Valid_citing_DOI\": valid_citing_doi,\n",
    "                    \"Invalid_cited_DOI\": invalid_cited_doi, \n",
    "                    \"Valid_DOI\": invalid_cited_doi,\n",
    "                    \"Already_valid\": 1\n",
    "                    })\n",
    "            else:\n",
    "                checked_dois.append(invalid_dictionary)       \n",
    "        else:\n",
    "            checked_dois.append(invalid_dictionary)\n",
    "        i += 1             \n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    return checked_dois"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python380jvsc74a57bd0b04ec2095feac21325ad77370b5a0b6e91c9a8b2c472b62b5d7ed091def7d874",
   "display_name": "Python 3.8.0 32-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "metadata": {
   "interpreter": {
    "hash": "b04ec2095feac21325ad77370b5a0b6e91c9a8b2c472b62b5d7ed091def7d874"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}